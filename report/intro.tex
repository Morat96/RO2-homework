\chapter{Introduction}

\section{Computational complexity theory}
First of all we want to start this report with a brief introduction to the computational complexity theory and in particular to the \textit{P} and \textit{NP}
classes. 
The computational complexity theory aim to establish how difficult is a certain problem. In order to do this, it is required a system to describe the complexity of the problem that has general validity, so that it is independent from the computer that we are using. For this reason an algorithm is studied with respect to the number of elementary operations that are required to complete it if it's executed by a Turing machine. So for example if we say that an algorithm is $O(n)$ where n is the size of the instance, it means that it requires in the worst case a number of operation to finish that it's linear with respect to the problem size itself. \\
We need to spend a few words for the Turing machine. For simplicity we can consider a deterministic Turing machine (DTM) the equivalent of a deterministic computer and a nondeterministic Turing machine (NTM) the equivalent of an ideal nondeterministic computer that is able to exploit multiple action at the same time. Please notice that this is a great simplification, an in-depth discussion about Turing machine and the computational complexity theory exceeds the purpose of this introduction, that is only to give a general idea. 
Now, in an informal way, we can say that a problem belong the \textit{P} class if exist an algorithm that resolve the associated decision problem in a polynomial time on a deterministic Turing machine. Moreover we say that a problem belong to the \textit{NP} class (nondeterministic polynomial) if exist an algorithm able to resolve the associated decision problem in polynomial time on a nondeterministic Turing machine. \\
A DTM can simulate a NTM, but in the worst case this require an exponential number of operation (again there is a theorem that guarantee this, omitted on this report). This means that a problem that belong to the \textit{NP} class, can require an exponential number of operation to be resolved on a deterministic computer, making it infeasible if the instance is not small. \\
There are no theoretical proofs that $P \neq NP$ (This is one of the \textit{Millennium Prize Problems}) so this mean that NP problems may be exponential only because we don't know an efficient algorithm to resolve them, however nowadays this is considered unlikely and it's widely believed that some problems are intrinsically exponential to resolve.\\
In conclusion there are some problem that are considered even more difficult than the ones that belong to the \textit{NP} class. These are called \textit{NP-hard} and they are of particular interest because lots of problem that have practical application in reality belong to this typology \cite{automi}. 

\section{Traveling salesman problem}
\label{TSPdef}
The traveling salesman problem (TSP) consist in finding the Hamiltonian path \footnote{In the mathematical field of graph theory, a Hamiltonian path (or traceable path) is a path in an undirected or directed graph that visits each vertex exactly once.} of lowest cost given an oriented graph $G=(V, A)$. The problem can be also defined in an analogous way on a non oriented graph if the cost of an edge doesn't depend  on the direction of the edge itself. This problem is very common in several real life situations, for example a courier that has to deliver parcels on different locations. However TSP is NP-hard and this is the reason why we can obtain exact solutions only for small instances (up to 600-800 nodes with the most advanced algorithms) while we have to look for different heuristic strategies if the instances are large \cite{ro}.
A possible integer linear programming model can be the following:

\begin{equation}
	\text{min} \underbrace{\sum_{(i,j) \in A} c_{ij}x_{ij}}_\text{circuit cost}
\end{equation}
\begin{equation}
	\underbrace{\sum_{(i,j) \in \delta^{-}(j)} x_{ij} = 1}_\text{one edge incoming in j}, \quad j \in V 
	\label{eqn:1.2}
\end{equation}
\begin{equation}
	\underbrace{\sum_{(i,j) \in \delta^{+}(i)} x_{ij} = 1}_\text{one edge outgoing from i}, \quad i \in V
	\label{eqn:1.3}
\end{equation}
\begin{equation}
	\underbrace{\sum_{(i,j) \in \delta^{+}(S)} x_{ij} \geq 1}_\text{subtour eliminator}, \quad S \subset V \ : \ 1 \in S
	\label{eqn:1.4}
\end{equation}
\begin{equation}
	x_{ij} \geq 0 \ \text{integer,} \ (i,j) \in A 
\end{equation}

where $x_{ij}$ are the decision variables defined in this way:
\[ x_{ij} =
	\begin{cases}
		1 \quad \text{if the edge} \ (i,j) \in A \ \text{is selected in the optimal circuit} \\
		0 \quad \text{otherwise}
	\end{cases}
\]

\noindent The \ref{eqn:1.2} means that the sum of all the edges entering a vertex must be equals to one, similar the \ref{eqn:1.3} says that the sum of all the outgoing edges in each vertex must be equals to one. So if we consider both \ref{eqn:1.2} and \ref{eqn:1.3} we have that each vertex must have only one incoming and one outgoing edge. \\ The \ref{eqn:1.4} says that each vertex $v$ must be reachable from the vertex 1, so that the solution must be connected and must visit all the vertices (subtour are not allowed). These constraints are $O(2^n)$, on the following chapters we will show some possible solutions to handle them.


\section{CPLEX}
CPLEX is an optimization software which is capable of solving Mixed Integer Programming (MIP) models (or quadratic convex) even of considerable size. The version used in our implementations is the 12.10.\\
CPLEX uses branch-and-cut search when solving MIP models. The branch-and-cut procedure manages a search tree consisting of \textit{nodes}. Every node of the tree represents an LP subproblem to be solved. Nodes not already processed are called active and remain in this state until they are reached and solved during the exploration of the branching tree. The branch-and-cut stops when no more active nodes are available or some limit has been reached.\\
A \textit{branch} is the creation of two new nodes from a parent node. Our reference problem is the TSP which has binary variables. In this case, if the current LP relaxation solution has a variable with a fractional value, the branching operation creates two child nodes: one node with a modified upper bound of 0 (requiring this variable to take only the value 0), and the other node with a modified lower bound of 1. The solution domains of the two child nodes are thus distinct.\\
In order to limit the size of the solution domain for the continuous LP problem, it is possible to add \textit{cuts} to the model, without eliminating the legal integer solutions. This makes possible to prune many branches of the decision tree, speeding up the resolution of the MIP.

\section{Performance Profile}
To compare our algorithms we used the Performance Profile \cite{dolan2002benchmarking}. For the comparison of algorithms it takes into account the number of problems solved as well as the cost it took to solve them. It scaled the cost of solving the problem according to the best solver for that problem. Given a set of problems $P$ and a set of algorithms $S$, we define $c_{s,p}$ as the cost of solving problem $p \in P$ by algorithm $s\in S$. If an algorithm can't solve the problem $p$, we define $c_{s,p} = +\infty$. We assume that at least one algorithm solves problem $p$. The best algorithm for a given problem is the one that solves it with the least cost, i.e., we define

\begin{equation}
	c_{min,p} = \min_{s \in S} c_{s,p}.
\end{equation}
\noindent 
Now we define the relative cost of the algorithm on a problem:

\begin{equation}
	r_{s,p} = {c_{s,p} \over c_{min,p}}.
\end{equation}

\noindent 
Notice that $r_{s,p} >= 1$, with $r_{s,p}=1$ meaning that algorithm $s$ is (one of) the best for problem $p$. Finally, the performance function of algorithm $s$ is given by

\begin{equation}
	P_s(t) = {\#\{p \in P \;|\; r_{s,p} <= t \} \over \#P}.
\end{equation}

\noindent 
See that $P_s(1)$ is the number of problems such that $r_{s,p} = 1$, that is the number of problems for which algorithm $s$ is one of the best. Furthermore, $P_s(r_{max})$ is the number of problems solved by algorithm $s$, where

\begin{equation}
	r_{max} = \max_{s \in S,\; p \in P} r_{s,p}.
\end{equation}
\noindent
The value $P_s(1)$ is called the efficiency of algorithm $s$ and $P_s(r_{max})$ is the robustness\footnote{For more informations on Performance Profile visit \url{http://abelsiqueira.github.io/blog/introduction-to-performance-profile/}}.

\begin{center}
\line(1,0){280}
\end{center}

\noindent 
In the next chapters we will present all our algorithms together with their implementations and tests. In particular, this report is structured as follows:

\begin{itemize}
  \item in Chapter 2 we will present some Compact models that we have studied and implemented, showing for each the mathematical formulation, a little description and a performance comparison between them;
  \item in Chapter 3 we will present Exact algorithms which can solve the TSP problem to optimum using the DJF formulation, handling the $O(2^n)$ Subtour Elimination Constraints (SECs) with different approaches;
    \item in Chapter 4 we will present two heuristic algorithms based on CPLEX that can handle problems with thousands of nodes;
    \item in Chapter 5 we will present some TSP solution builders and a variety of metaheuristic algorithms;
     \item in Chapter 6 we will present the conclusions of our work and references;
     \item in Chapter 7 we have collected the Result Tables of the tests submitted to our algorithms.
\end{itemize}

All the source code we developed is available at:
\begin{center}
\url{https://github.com/Morat96/RO2-homework} \footnote{For more informations on how compile and run the code, see the README.}. 

\end{center}

